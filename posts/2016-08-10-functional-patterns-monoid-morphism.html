<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
    <title>Badness 10.000 - Algebraic Patterns - Monoid morphisms</title>
    <link rel="stylesheet" type="text/css" href="../css/default.css" />
    <link rel="stylesheet" type="text/css" href="../css/pandoc.css" />
    <link rel="alternate" type="application/rss+xml" href="http://philipnilsson.github.io/Badness10k/atom.xml" title="RSS feed for this page" />

  </head>
  <body>
    <div id="header">
      <div id="logo">
        <a href="../">Badness 10.000</a>
      </div>
      <div id="navigation">
        <a href="../">About</a>
      </div>
    </div>

    <div id="content">
      <h1>Algebraic Patterns - Monoid morphisms</h1>

      <div class="info">
    Posted on August 10, 2016
    
</div>

<script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<p>\[ \newcommand\emptyList{\texttt{[]}} \newcommand\doubleplus{\kern0.8ex\texttt{++}\kern0.8ex} \newcommand\composition{\kern0.8ex\texttt{&lt;&gt;}\kern0.8ex} \newcommand\doubleplusop{\kern0.8ex\texttt{++}_{op}\kern0.8ex} \]</p>
<h2 id="introduction">Introduction</h2>
<p>In this instalment we’ll take a further look at <strong>Monoid morphisms</strong> to try to cement our understanding of this concept. We’ve used monoids to understand parallelism in the Map-Reduce style programming model. We’ll continue by looking at applications in the related notion of <em>Divide &amp; Conquer</em> style problem solving.</p>
<p>To re-iterate, monoid morphisms are structure-preserving maps between <a href="./2016-07-21-functional-patterns-monoid.html">monoids</a>. That is for two monoids <span class="math inline">(<em>M</em>, ⊕, <em>e</em>)</span> and <span class="math inline">(<em>N</em>, ⊗, <em>f</em>)</span> a monoid morphism <span class="math inline"><em>h</em></span> is a function satisfying</p>
<p>\[ \begin{aligned} h(e) &amp; = f\\ h(a \oplus b) &amp; = h(a) \otimes h(b) \end{aligned} \]</p>
<p>so a monoid morphism preserves identity and composition.</p>
<h2 id="divide-conquer">Divide &amp; Conquer</h2>
<p>Below we will solve at a set of <a href="https://en.wikipedia.org/wiki/Divide_and_conquer">Divide &amp; Conquer</a> type problems and show that monoid morphisms often capture the divide and conquer steps.</p>
<p>Wishing to calculate the result of applying a function <span class="math inline"><em>h</em></span>, we start by “dividing” it’s argument by writing it as a composition in terms of <span class="math inline">⊕</span>.</p>
<p>\[h(a \oplus b)\]</p>
<p>and by using the monoid morphism law, we rewrite in terms of <span class="math inline">⊗</span></p>
<p>\[h(a) \otimes h(b)\]</p>
<p>which captures the “conquering” — a re-composition step that recombines solved subproblems <span class="math inline"><em>h</em>(<em>a</em>)</span> and <span class="math inline"><em>h</em>(<em>b</em>)</span> into a full solution.</p>
<h2 id="sorting">Sorting</h2>
<p>There is the ordinary monoid of lists, but there is a also different monoid of <em>sorted</em> lists. The monoid of sorted lists does not have the usual concatenation <span class="math inline">⧺</span> as it’s composition, as it does not “maintain sortedness”, i.e. <span class="math inline"><em>x</em><em>s</em> ⧺ <em>y</em><em>s</em></span> need not be sorted just because <span class="math inline"><em>x</em><em>s</em></span> and <span class="math inline"><em>y</em><em>s</em></span> are.</p>
<p>Instead we’ll use as composition the operation <span class="math inline"><code>merge</code></span> which we’ll denote with <span class="math inline">⊙</span>. It is clearly associative, and has the empty list as its identity.</p>
<p>The function <span class="math inline"><em>s</em><em>o</em><em>r</em><em>t</em></span> is a monoid morphism from ordinary lists to the monoid of sorted lists</p>
<p>\[ \begin{aligned} sort(\emptyList) &amp; = \emptyList\\ sort(xs \doubleplus ys) &amp; = sort(xs) \odot sort(ys) \end{aligned} \]</p>
<p>If <span class="math inline"><em>s</em><em>o</em><em>r</em><em>t</em></span> is an <span class="math inline"><em>n</em><sup>2</sup></span> time sort, such as insertion-sort or bubble-sort, this monoid morphism encodes an optimization step. Since merging can be done in linear time, and the decomposition <span class="math inline"><em>x</em><em>s</em> ⧺ <em>y</em><em>s</em></span> can be chosen to halve the length of the composite list, we can go from <span class="math inline"><em>n</em><sup>2</sup></span> to \((2(\frac{n}{2})^2 + n)\) time, which is a significant performance improvement.</p>
<p>By repeatedly applying this monoid morphism we derive the merge-sort algorithm.</p>
<h2 id="exponentiation">Exponentiation</h2>
<p>The function <span class="math inline"><em>e</em><em>x</em><em>p</em></span> is a monoid morphism from the monoid of numbers with addition to the monoid of numbers with multiplication. The meaning of this statement is that <span class="math inline"><em>e</em><em>x</em><em>p</em></span> should satisfy</p>
<p>\[ exp(0) = 1\\ exp(b + c) = exp(b) * exp(c) \]</p>
<p>which we know to be true, since</p>
<p>\[ exp(a + b) = e^{a + b} = e^a e^b = exp(a) * exp(b) \]</p>
<p>This argument would work for any exponentiation function, not just with <span class="math inline"><em>e</em></span> as a base, so there is a monoid morphism of this form for any number <span class="math inline"><em>a</em></span>, namely the function <span class="math inline"><em>x</em> ↦ <em>a</em><sup><em>x</em></sup></span>.</p>
<p>\[ a^0 = 1\\ a^{b + c} = a^b * a^c \]</p>
<p>We can take advantage of this identity if we’d like to calculate an exponential where the exponent is an even whole number.</p>
<p>\[ a^{2n} = a^{n} * a^{n} \]</p>
<p>Assuming we’re calculating this exponent in the naive way of performing <span class="math inline">2<em>n</em></span> multiplications, this rule describes an optimization step, resulting instead in <span class="math inline"><em>n</em> + 1</span> multiplications, as we need only calculate <span class="math inline"><em>a</em><sup><em>n</em></sup></span> a single time.</p>
<p>A similar optimization can be performed for an odd whole number, leading to an <a href="https://en.wikipedia.org/wiki/Exponentiation_by_squaring">algorithm using <span class="math inline"><em>l</em><em>o</em><em>g</em><sub>2</sub>(<em>n</em>)</span> multiplications</a>.</p>
<h2 id="counting-zeroes">Counting zeroes</h2>
<p>In interview question I came across ask one to find the number of trailing zeroes in a large factorial, let’s say <span class="math inline">100!</span>, without calculating this actual value, as it is very large.</p>
<p>Again, this problem can be solved through a divide &amp; conquer style of problem solving using monoid morphisms.</p>
<p>There is a monoid morphism from numbers with multiplication, to the <a href="http://localhost:8000/posts/2016-07-21-functional-patterns-monoid.html">pointwise lifted</a> monoid of numbers with addition.</p>
<p>The fact that such a morphism exists is known as the Fundamental Theorem of Arithmetic, that is, we can map a number to its prime factorization.</p>
<p>\[ primes(1) = \emptyset\\ primes(a * b) = primes(a) \cup_+ primes(b) \]</p>
<p>Here <span class="math inline">∪<sub>+</sub></span> denotes pointwise addition in the set of prime frequencies.</p>
<p>By applying this monoid morphism, we can find the prime factorization of <span class="math inline">100!</span>.</p>
<p>\[ \begin{aligned} &amp; primes(100!) \\ = \enspace &amp; primes(100 * 99!) \\ = \enspace &amp; primes(100) \cup_+ primes(99!) \\ = \enspace &amp; primes(100) \cup_+ primes(99) \cup_+ \ldots \cup_+ primes(2) \end{aligned} \]</p>
<p>From this result it is easy to find the number of zeroes in <span class="math inline">100!</span>. We simply take the max of the prime frequency of 2 and 5. Since finding the prime factorizations up to <span class="math inline">100</span> is quickly done, this again describes an optimization.</p>
<p>One might note that we can in fact solve this problem even faster, by simply counting the prime frequencies for 2 and 5 <em>only</em>. In order to formulate this simplification in the language of monoid morphisms we would need to introduce equivalence classes, and their interactions with monoid structure, which we will not do or explain here.</p>
<p>We note that any partitioning of a set over an equvalence relation induces a monoid morphism to the set of partitions as long as the composition respects the equivalence. We challenge the interested reader to find out more.</p>
<h2 id="de-morgans-laws">De Morgan’s Laws</h2>
<p>\[ \newcommand\andand{\kern0.8ex\texttt{&amp;&amp;}\kern0.8ex} \newcommand\oror{\kern0.8ex\texttt{||}\kern0.8ex} \]</p>
<p>The negation function <span class="math inline"><code>!</code></span> which negates a boolean proposition is a monoid morphism from <span class="math inline">(<em>B</em><em>o</em><em>o</em><em>l</em>, <code>&amp;&amp;</code>, <code>true</code>)</span> to <span class="math inline">(<em>B</em><em>o</em><em>o</em><em>l</em>, <code>||</code>, <code>false</code>)</span>.</p>
<p>\[ \begin{aligned} \texttt{!}\texttt{true} =&amp; \enspace \texttt{false}\\ \texttt{!}(a \andand b) =&amp; \enspace !a \oror !b \end{aligned} \]</p>
<p>It is also a monoid morphism going the other way, from <span class="math inline">(<em>B</em><em>o</em><em>o</em><em>l</em>, <code>||</code>, <code>false</code>)</span> to <span class="math inline">(<em>B</em><em>o</em><em>o</em><em>l</em>, <code>&amp;&amp;</code>, <code>true</code>)</span></p>
<p>\[ \begin{aligned} \texttt{!}\texttt{false} =&amp; \enspace \texttt{true}\\ \texttt{!}(a \oror b) =&amp; \enspace !a \andand !b \end{aligned} \]</p>
<p>The fact that <span class="math inline"><code>!</code></span> is a monoid morphism in these ways is known as De Morgan’s laws.</p>
<h2 id="stable-sorts">Stable sorts</h2>
<p>Stable sorting functions are monoid morphisms from the monoid of comparators to the monoid of functions with function-composition. Let <span class="math inline"><em>s</em><em>o</em><em>r</em><em>t</em><sub><em>c</em></sub></span> be the sorting function that sorts by comparator <span class="math inline"><em>c</em></span>.</p>
<p>Let <span class="math inline"><em>ε</em></span> be the identity-comparator, that compares all elements as equal, and <span class="math inline">⋄</span> be <a href="./2016-07-21-functional-patterns-monoid.html">comparator-composition</a>. Then a stable sort satisfies</p>
<p>\[ sort_\varepsilon = id \\ sort_{c \thinspace \diamond \thinspace d} = sort_c \circ sort_d \]</p>
<p>this encodes the observation that for a stable sort we can fuse two sorts by different comparators <span class="math inline"><em>c</em></span> and <span class="math inline"><em>d</em></span> into a single sorting pass that compares first by <span class="math inline"><em>c</em></span> and then by <span class="math inline"><em>d</em></span>.</p>
<h2 id="the-opposite-monoid">The opposite monoid</h2>
<p>A weird but interesting construction on monoids is the opposite monoid. It is constructed from a given monoid by taking</p>
<p>\[ x \otimes_{op} y = y \otimes x \]</p>
<p>so this monoid is the same except the order of arguments is the reverse of the target monoid. The identity element stays the same.</p>
<hr>
<p>The function <span class="math inline"><em>r</em><em>e</em><em>v</em><em>e</em><em>r</em><em>s</em><em>e</em></span> is an example of a monoid morphism. It goes from the monoid of lists to the opposite monoid of lists — this means it will satisfy the monoid morphism laws</p>
<p>\begin{equation} reverse(\emptyList) = \emptyList\\ \begin{aligned} &amp; reverse(xs \doubleplus ys)\\ = \enspace &amp; reverse(ys) \doubleplus reverse(xs)\\ = \enspace &amp; reverse(xs) \doubleplusop reverse(ys) \end{aligned} \end{equation}</p>
<p>It it helps, here’s an example with concrete values</p>
<p>\[ \begin{aligned} reverse(\texttt{[1, 2, 3, 4, 5]}) &amp; =\\ reverse(\texttt{[1, 2, 3] ++ [4, 5]}) &amp; = \\ reverse(\texttt{[4, 5]}) \doubleplus reverse(\texttt{[1, 2, 3]}) &amp; =\\ reverse(\texttt{[1, 2, 3]}) \doubleplusop reverse(\texttt{[4, 5]}) &amp;= \\ \texttt{[5, 4] ++ [3, 2, 1]} \end{aligned} \]</p>
<h2 id="word-reversals">Word reversals</h2>
<p>A common interview-question consists of reversing the order of words in a string without using any extra memory, a so-called in place algorithm.</p>
<p>\[ reverseWords(\texttt{“one two three”}) = \texttt{“three two one”} \]</p>
<p>Since the function <span class="math inline"><em>r</em><em>e</em><em>v</em><em>e</em><em>r</em><em>s</em><em>e</em></span> is easily written as an in-place algorithm, it is possible to solve this problem by expressing a solution as some number of substring reversals on the input string.</p>
<p>We can construct such an expression by starting with the solution and working backwards until the character appears in their original order. We’ll make use of the monoid morphism laws.</p>
<p>\[ \begin{aligned} &amp; reverseWords(\texttt{“one two three”})\\ = \enspace &amp; \texttt{“three two one”}\\ = \enspace &amp; reverse(reverse(\texttt{“three two one”}))\\ = \enspace &amp; reverse(reverse(\texttt{“three” ++ &quot; two &quot; ++ &quot; one“}))\\ = \enspace &amp; reverse(reverse(\texttt{”three“}) \doubleplusop reverse(\texttt{” two “}) \doubleplusop reverse(\texttt{”one“}))\\ = \enspace &amp; reverse(reverse(\texttt{”one“}) \doubleplus reverse(\texttt{” two “}) \doubleplus reverse(\texttt{”three“}))\\ \end{aligned} \]</p>
<p>So an in-place algorithm for word-reversal can be found by reversing each individual word in a string, then reversing the entire string.</p>
<h2 id="string-cycling">String cycling</h2>
<p>A related problem to the above is the problem of cycling strings.</p>
<p>\[ cycle_2(\texttt{“abcdef”}) = \texttt{“cdefab”} \]</p>
<p>that is, <span class="math inline"><em>c</em><em>y</em><em>c</em><em>l</em><em>e</em><sub><em>n</em></sub></span> will “rotate” <span class="math inline"><em>n</em></span> characters from the front of the string to the end.</p>
<p>This problem too can be made into a tricky interview question by asking to perform it in-place, but we can conquer such difficulties in a similar way to the above.</p>
<p>\[ \begin{aligned} &amp; cycle_3(\texttt{“abcdefgh”})\\ = \enspace &amp; \texttt{“defghabc”}\\ = \enspace &amp; reverse(reverse(\texttt{“defghabc”}))\\ = \enspace &amp; reverse(reverse(\texttt{“defgh” ++ “abc”}))\\ = \enspace &amp; reverse(reverse(\texttt{“defgh”}) \doubleplusop reverse(\texttt{“abc”}))\\ = \enspace &amp; reverse(reverse(\texttt{“abc”}) \doubleplus reverse(\texttt{“defgh”}))\\ \end{aligned} \]</p>
<p>and so cycling a list in place by <span class="math inline"><em>n</em></span> places can be achieved by reversing the first <span class="math inline"><em>n</em></span> elements, then the rest of the list, and finally reversing the entire list again.</p>

    </div>
    <div id="footer">
      <div>Site proudly generated by <a href="http://jaspervdj.be/hakyll">Hakyll</a></div>
      <a type="application/rss+xml" href="http://philipnilsson.github.io/Badness10k/atom.xml">RSS feed for this page</a>

    </div>
  </body>
</html>
